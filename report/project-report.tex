\documentclass[12pt,a4paper]{report}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{booktabs}
\usepackage{xcolor}
% \usepackage{titlesec}
\usepackage{parskip}
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% Code listing settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{xcolor}

% Define inline code styling
\lstdefinestyle{inlineCode}{
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    columns=flexible
}

% Define custom inline code command
\newcommand{\code}[1]{\lstinline[style=inlineCode]!#1!}

\title{Peer-to-Peer File Sharing System:\\A Robust and Scalable Implementation}
\author{Yugal Khanal\\2302704}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	The proliferation of distributed systems has led to increased interest in peer-to-peer (P2P) architectures for file sharing. This dissertation presents the design, implementation, and evaluation of a robust P2P file sharing system that addresses key challenges in scalability, fault tolerance, and security. The system implements a hybrid architecture combining centralized tracking with distributed file storage, featuring chunked file transfer, piece verification, and concurrent downloading capabilities.

	The implementation includes sophisticated features such as tracker-based peer discovery, UPnP port mapping for NAT traversal, and a comprehensive piece management system for handling large file transfers. Through extensive testing and evaluation, the system demonstrates reliable performance under various network conditions while maintaining data integrity and transfer efficiency.

	This work contributes to the field by implementing novel approaches to common P2P challenges, including peer availability management and fault-tolerant file transfers, while providing insights into the practical considerations of building distributed systems.

	\textbf{Keywords:} Peer-to-Peer Networks, Distributed Systems, File Sharing, Network Programming, Fault Tolerance
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\chapter{Introduction}
\section{Background and Motivation}
The increase is the amount of Peer-to-Peer (P2P) file-sharing systems has changed digital content distribution. It's main advantage over it's traditional counterpart client-server model where a central authority manages file transfers is that P2P networks distribute this responsibility over to the peers in the network significantly increasing scalability and fault tolerance.

Data consumption has had a boom in recent years and because of the limitations of the client-server architecture such as high bandwidth costs, central points of failure etc., there has been a demand for efficient, secure and scalable file-sharing mechanisms. Modern P2P networks are the perfect solution to this. They incorporate advanced networking protocols, cryptographic security, and optimisation algorithms to enhance their performance and security. Enhanced scalability is achieved by distributing the file storage and transfer responsibilities among the peers in the network. This reduces the load on individual nodes and improves the overall efficiency of the system. Data redundancy and availability are also improved by storing multiple copies of the file across the network. These two are very strong points for P2P networks and any data distribution system would benefit from these features.

P2P networks have wider applications beyond just simple file-sharing. It's applications range from content distribution networks (CDNs) to blockchains and distributed cloud computing. There are also significant challenges in this approach. Some of them being data security, peer reliability and performance bottlenecks. This project aims to address some of these challenges by designing and implementing a secure, scalable and fault-tolerant P2P file-sharing system.


\section{Project Objectives}
 [Clear enumeration of project goals and success criteria]

The main objective of this project is to design and implement a robust and scalable P2P file-sharing system that addresses key challenges in scalability, fault tolerance, and security. The system will be evaluated based on the following criteria:

\begin{itemize}
	\item Develop a decentralised file-sharing system: The system should be able to handle a large number of peers and files while maintaining performance.
	\item Fault Tolerance: The system should be resilient to peer failures and network disruptions, ensuring reliable file transfers.
	\item Security: The system should implement secure communication protocols and data encryption to protect user data.
	\item Performance: The system should provide efficient file transfer mechanisms and low latency for peer interactions.
	\item Implement secure encryption mechanisms to safeguard data integrity.
\end{itemize}


\section{Evaluation Criteria}
 [Description of the metrics and methods used to evaluate the system]

\begin{itemize}
	\item System Performance: The system's performance will be evaluated it terms of latency, download speeds, system response and throughput efficiency under various network conditions.
	\item Scalability: The system's ability to handle increasing numbers of simultaneous connections and file-sharing requests without performance degradation.
	\item Fault Tolerance: Evaluation of the system's resilience to peer failures and disconnections, network disruptions, and data corruption.
	\item Security: Effectiveness of the implemented encryption algorithms, peer authentication mechanisms and safeguards against malicious attacks.
	\item Resource Utilization: The system's resource consumption in terms of CPU, memory, and network bandwidth. How efficient the system is in using bandwidth and system resources.
\end{itemize}

By evaluating the system based on these criteria, we can determine the effectiveness of the implemented features and the overall performance of the P2P file-sharing system.
This project will hopefully also contribute to the advancement and highlight the importance of decentralized technologies.

\section{Problem Statement}
 [Detailed description of the challenges in P2P file sharing]

\section{Project Scope}
 [Outline of what the project encompasses and its boundaries]
Peer-to-peer file sharing is the distribution

\chapter{Literature Review}
\section{History of P2P Systems}
Early peer-to-peer (P2P) file sharing systems showed up in the late 1990s and evolevd rapidly throught the 2000s. Everything started with Napster (1999), created by Shawn Fanning, that allowed users to share music files (MP3) \cite{wikipedia-p2p}. Napster used a centralised server to index available files while the actual file transfers were done directly between user computers (peers). This centralised index made Napster easy to use and very popular. At its peak, Napster had 80 million registered users \cite{wikipedia-napster} which also became its downfall as it was down in 2001 for copyright infringement.

Following Napster, Gnutella was released in 2000 as the very first fully decentralised P2P file sharing network. With Gnutella, the centralised index server was gone; instead, each peer acted equally as a client and a server. The file requests were broadcasted to all peers. At the start, this was great for reliability. However, scaling became a problem as the network grew as flooding each query to all peers caused high overhead and slow searching. Around the same time, other systems like FastTrack (Kazaa, 2001) which became the most popular P2P protocol at the time, used supernodes as indexes to improve scalability. These supernodes were a subset of peers with high-bandwidth that acted as a proxy for other peers \cite{wikipedia-fasttrack}. This combined the central index's efficiency with a decentralised system for actual file transfers. This reduced the overhead of flooding queries to all peers. Similarly, eDonkey2000 (2000) relied on multiple servers for indices, and it's later client eMule (2002) implemented a Kademlia distributed hash table (Kad network) to enable serverless operation.

By the early 2000s,P2P networks had many versions. Some with unstructured networks like Gnutella, structured networks with distributed indices, and hybrid models. The early success and failure of Napster allowed for new P2P file-sharing networks and their software client counterparts \cite{early-2000s-p2p-state}. Since the newer P2P systems don't rely on any central server by using a Distributed Hash Table (DHT), it is much harder for law enforcements to shut it down. The concept of P2P networks has been around for long time. The fundamental Internet Protocols like TCP/IP with its applications in FTP, HTTP, etc. are all based on host-to-host communication where all hosts can request data from all other hosts \cite{early-2000s-p2p-state}. P2P networks are an evolution/extention of these principles, building application-level networks on top of the Internet that directly connect users to each other to freely share any kind of data.

BitTorrent was the biggest milestone in P2P network history, introduced by Bram Cohen in 2001. BitTorrent's totally new and innovative approach to file sharing - discussed in more detail in the next section - immensely improved scalability and efficiency for distributing large files. By utilising swarms of users to concurrently download and upload pieces of a file, BitTorrent was able to reduce the load on individual peers and increase the overall speed of file transfers. This made BitTorrent the most dominant P2P file sharing protocol by the mid-2000s and is still the most popular P2P protocol today \cite{BitTorrent-most-popular}. To summarise, the evolution of P2P file sharing moved from a central index model like Napster, to a purely decentralised but inefficient model like Gnutella, to hybrid and structured network like FastTrack and eDonkey2000, and finally ending up in the efficient and scalable BitTorrent model. These advancements in P2P technology were driven by the need for a lack of central bottlenecks, improved discovery and download speeds and increase reliability (robutness) of the file sharing networks agains failures and legal threats.


\section{BitTorrent Protocol Analysis}
BitTorrent is a peer-to-peer protocol designed for efficient distribution of large files by utilising swarms (the collective group of peers sharing a particular file) to download and upload pieces of the file concurrently. Under the hood, BitTorrent breaks each file into many small pieces called chunks and distributes them among the peers in a swarm, so all participants can download and exchange different pieces with each other simultaneously \cite{bep_0003}. A user requires a torrent descriptor file (.torrent) or a magne link, which contains metadata including a unique info-has identifies for the content and optionally addresses of tracker servers - coordinatin servers that help peers fine each other \cite{bep_0003}. When any BitTorrent client loads this metadata, it joins the swarm by contacting the tracker server or using a Distributed Hash Table (DHT) to discover other peers. Peers in the swarm that have the complete files are called seeders, while those that are still downloading are called leechers or just peers. When the leecher downloads any piece, it begins to upload that piece to others immediately after, contributing to the swarm. This design means that the content distribution load is shared among all peers, and the more peers there are, the faster the download speed can be. With enough peers, a file's original source only needs to provide one full copy, after which the swarm can continue to share the file without the original source's involvement. This decentralised sharing content makes BitTorrent highly scalable.

In a BitTorrent network, at the start, file pieces are downloaded in a strategic order using the \textit{rarest-first}. Peers prioritise requesting pieces that are rarest in the swarm (pieces that fewest other peers have), making those pieces more widely replicated early to ensure availability. This strategy ensures that no single piece/peer becomes a bottleneck in the swarm. BitTorrent also implements a tit-for-tat inspired choking algorithm to encourage cooperation and as it's incentive mechanism. In practice, each client monitors its other peers' upload/download rates and "chokes" (temporarily stops sending data to, downloading can still take place) peers that are not sharing, which unchoking peers that do send data \cite{choking}. Consequently, a peer that uploads more is allowed to download more from others. This leads to faster overall performance for peers that share. This tit-for-tat mechanism is crucial in discouraging free-riders and keeps the swarm healthy. BitTorrent also uses \textit{optimistic unchoking} to randomly unchoke some connections, allowing new and temporarily idle peers to gain pieces \cite{choking}. This also gives a chance to previously choked peers to state that they are willing to share. Because there is no central scheduler, each peer automatically maximises its own download rate by selectively uploading to partners that reciprocate. Doing this also stabilises the download speed, giving peers a consistent download rate.

BitTorrent's swarming approach of downloading pieces from multiple peers in parallel means that the collective bandwidth of the swarm increases as more peers join, greatly improving download efficiency. To put it simply, the swarm's average throughput scales with the number of peers - a sharp contrast and advantage to traditional client-server downloads limited by a server's capacity.

Initially, BitTorrent relied on centralised servers/trackers for peer discovery, but this presented reliability and availability risks (if a tracker went offline, the swarm could become unreachable). To address this, a \textbf{Distributed Hash Table (DHT)} system was introduced (BitTorrent Enhancement Protocol (BEP) 5) to decentralise peer discovery. The DHT is composed of nodes and stores the location of peers. BitTorrent clients include a DHT node, which is used to contact other nodes in the DHT to get the location of peers to download from using the BitTorrent protocol \cite{bep_0005}. The DHT is based on the Kademlia algorithm. In Kademlia, the distance metric is XOR and the result is interpreted as an unsigned integer. distance(A, B) = $|\text{A xor B}|$ smaller values are closer. The distance metric is used to compare two node IDs or a node ID and an infohash for "closeness" \cite{bep_0005}. Peers publish and retrieve torrent swarm information on the DHT by using the torrent's infohash as the key. To find peers for a torrent, a node performs \texttt{get\_peers} query on the DHT with the infohash. The DHT nodes responsible for that key will respond with a list of IP and ports for peers in the swarm. For any peer joining the swarm, the peer uses \texttt{announce\_peer} along with the torrent infohash to announce its presence to the DHT so other nodes can find it \cite{bep_0005}. This way the DHT functions as a decentralised "tracker", allowing peers to find each other without any central server.

In addition to the DHT, BitTorrent also utilises \textbf{Peer Exchange (PEX)} and \textbf{Local Peer Discovery (LDP)} to further improve peer discovery. PEX is an alternative peer discovery mechanism for swarms once peers have bootstrapped via other mechanisms such as DHT \cite{bep_0011}. PEX allows peers to directly exchange peer lists with each other, enabling them to discover new peers quickly without needing a tracker or DHT. LDP allows peers on the same local network (on the same LAN) to discover each other using multicast (http over udp-multicast) \cite{bep_0014}. It's aim is to minimise the traffic through the ISP's channel use the higher LAN bandwidth instead. Together with the DHT, PEX and LDP provide a robust and decentralised peer discovery mechanism for BitTorrent.

Through the combination of efficient piece distribution, tit-for-tat incentive mechanism (choking/unchoking), and multiple peer discovery methods, BitTorrent achieves both very high performance and reliability. BitTorrent remains the most widely used P2P protocol today having inspired many other distributed systems.

\section{Modern P2P Applications}
Although P2P technology began with file sharing, today it is used in a diverse array of modern applications beyond just simple file sharing. One such domain is \textbf{Blockchain} and \textbf{Cryptocurrencies}. They fundamentally operate as P2P networks. In the original white paper for Bitcoin, introduced in 2008, it is described as a "peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions" and a Bitcoin to be an electronic coin which is a chain of digital signatures. In a Blockchain network like Bitcoin, each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin without a central authority \cite{bitcoin}. Each node also maintains a copy of the ledger and participates in a proof-of-work protocol to agree on the next block of transactions. The proof-of-work involves finding a value that, when hashed with the block's data, produces a hash with some number of leading zeros \cite{bitcoin}. This decentralised design makes the system resilient and resistant to censorship and attacks. It even has an incentive mechanism to encourage participation in the form of newly minted coins or transaction fees, thereby keeping the network running smoothly. Multiple decentralised applications have been built on blockchain P2P networks (Ethereum being the most popular).

Decentralised data storage and content distribution is another area where P2P networks are used. IPFS (InterPlanetary File System) is a notable example of modern P2P protocol for distributed file storage. It's a protocol and network designed to create a content-addressable, peer-to-peer method of storing and sharing hypermedia in a distributed file system \cite{IPFS}. It forms a global, decentralised file system in which content is addressed by a unique hash. Upon a request for content, the IPFS network uses a DHT to locate peers that have the content and retrieves it from the fastest available sources. This approach allows content to be shared across multiple nodes, making content delivery efficient and reliable (files remain available even if some nodes disconnect, just like in BitTorrent). Many newer applications, especially Web3 applications, leverage IPFS or similar P2P protocols for content distribution without relying on a central server.

From file-sharing to cryptocurrencies and decentralised storage, modern P2P networks demonstrate the versatility and power of decentralised systems. By removing central servers, these applications gain fault-tolerance, increased performance and availability. The principles of P2P networking have been applied to a wide range of applications, and the technology continues to evolve with new use cases and innovations.

\section{Security Challenges in P2P Networks}
Along with the various advantages of P2P networks, they also present significant security and privacy challenges. It's decentralised nature and direct peer-to-peer communication makes enforcing security policies and protecting users more difficult compared to traditional centralisd systems.

\textbf{Privacy and Anonymity} is the primary challenge. In P2P networks, peers' IP addresses are exposed to others in the newtork, meaning user activiy can be monitored. True anonymity is difficult to achieve in P2P networks, as the IP addresses of peers must be published in order to connect and exchange data. This makes it easy for organisations to join swarms to log peer IP addresses and ban those IPs from joining P2P newtorks. Users can use VPNs or other IP masking techniques to hide their true IP address, but this merely shifts the trust to the VPN provider and is not foolproof. As a result, traditional P2P networks offer little built-in anonymity. Some specalised networks emerged to address this issue, like I2P, Freenet, and Tor etc. These networks route traffic through multiple nodes to obfuscate the source and destination of data, providing a higher level of privacy and anonymity. However, they trade off efficiency and are not widely adopted for general file sharing.

\textbf{Malware and Content Integrity} is another major concern. P2P networks are often used to distribute pirated content, which can be a vector for malware. Malicious users can upload malware disguised as legitimate files with no party checking it for authenticity. Users can unknowingly download these files and install malware thinking it is a legitimate file. P2P networks can become a breeding ground for trojans and viruses because of the ease of injection. Attacker can also flood the networks with fake or corrupted files due to the lack of verification \cite{security} \textbf{RECHECK THISSSSSSSSSSSSSSS}.

\chapter{System Design and Implementation}
\section{Architecture Overview}
% High-level design and key components
% Design principles and patterns
% Development environment (VMs, networking setup, etc.)

This P2P file sharing system implements a hybrid architecture that combines \textbf{centralised tracking} for peer discovery with \textbf{decentralised file transfer} among peers. This design provides the scalability benefits of distributed systems while maintaining efficient peer discovery through a central coordinator. At a high level, the system consists of the following core components:

\begin{itemize}
	\item \textbf{Tracker:} A lightweight centralised server that maintains metadata about files and coordinates peer discovery. The tracker does not store any file content and does not participate in file transfers, only facilitating peer connections.
	\item \textbf{File Server:} The core component of the system running on each peer that handles file storage, chunking, storage and downloading. It implements BitTorrent inspired piece management and chunking strategies for efficient file transfers.
	\item \textbf{Transport Layer:} A dual protocol system that supports both TCP and UDP for optimal file transfers and NAT traversal.
	\item \textbf{Storage System:} Manages the persistent storage of file chunks and metadata on disk. Maintains data integrity and verification mechanisms along with content addressing and file reconstruction.
	\item \textbf{Piece Manager:} Coordinates the parallel downloading and verification of individual file chunks using a variety of piece selection strategies for optimal performance.
\end{itemize}

File transfers are inspired by BitTorrent's approach, where files are divided into chunks, allowing for parallel downloading from multiple peers simultaneously. Each chunk is hashed and verified to maintain data integrity. The combination of all core components ensures a robust and scalable file-sharing experience for users.

Certain design principles and patterns dictated the system's architecture:
\begin{itemize}
	\item \textbf{Fault Tolerance:} The system is designed to be resilient to peer failures and network disruptions. The system can handle peer disconnections, network failures, and data corruption gracefully through techniques like retry logic, error recovery, and peer connection strategies.
	\item \textbf{Scalability:} By distributing file storage and transfer responsibilities among peers themselves, the system can scale to handle many simultaneous users and large files with no central bottleneck.
	\item \textbf{Performance Optimisation:} The system includes optimisations like parallel downloads, piece prioritisation (rarest first), etc. to maximise file transfer speeds.
	\item \textbf{Protocol Flexibility:} The dual TCP and UDP transport layer allows the system to adapt to different network conditions when NAT traversal is required.
	\item \textbf{Data Integrity:} \sloppy{Hashing and verification mechanisms are implemented to ensure that file chunks are not corrupted during transfer and tampered with.}
\end{itemize}

The system was developed in Go (Golang), chosen for its efficiency, excellent concurrency model with goroutines and channels, strong standard networking (among others) libraries, and cross-platform compatibility. The development environment consisted of:
\begin{itemize}
	\item Go 1.23 for the programming language on the local machine running NixOS(btw).
	\item Multiple virtual machines (VMs) running Ubuntu to simulate a distributed network.
	\item Network configuration with various NAT setups to test NAT traversal.
	\item Comprehensive logging for debugging and monitoring.
\end{itemize}

\section{Tracker Component}
% Design considerations
% Data structures and algorithms
% HTTP endpoints and protocol
% Implementation challenges and solutions

The tracker servers as a lightweight coordination service that helps peers discover each other and share file metadata. It maintains an index of available files and the peers sharing them.

\section*{Design Considerations}
The tracker was designed with these key considerations:
\begin{itemize}
	\item \textbf{Minimal State:} The tracker stores only essential information needed for peer discovery, not the file itself.
	\item \textbf{Availability:} As a central component, the tracker needs to be highly available.
	\item \textbf{Scalability:} The tracker must efficiently handle many concurrent requests from peers.
	\item \textbf{Lightweight:} HTTP was chosen for simplicity and compatibility.
\end{itemize}

As shown in the Listing~\ref{lst:tracker-code}, the several key data structures used allow efficient lookup of files by ID and tracking of which peers have which files.

\section*{HTTP Endpoints}
The tracker exposes several HTTP endpoints for peers:

\begin{enumerate}
	\item \textbf{/announce:} Used by peers to announce they are sharing a file.
	\item \textbf{/peers:} Returns a list of peers sharing a particular file.
	\item \textbf{/metadata:} Returns detailed metadata about a file in JSON format.
	\item \textbf{/files:} Lists all available files.
	\item \textbf{/remove:} Allows peers to indicate they've stopped sharing a file.
	\item \textbf{/heartbeat:} Used by peers to maintain their active status.
\end{enumerate}

The core announce handler in the Listing~\ref{lst:handle-announce} demonstrates how peers register files.

\section*{Peer Management}
The tracker implements an active cleanup process to maintain an accurate list of available peers:

\begin{lstlisting}[language=Go, caption=Peer Cleanup]
func (t *Tracker) StartCleanupLoop() {
  config := DefaultCleanupConfig()
  ticker := time.NewTicker(config.CleanupInterval)
  go func() {
      for range ticker.C {
          t.cleanupInactivePeers(config.InactivityThreshold)
      }
  }()
}
\end{lstlisting}

This periodic cleanup process removes peers that haven't sent a heartbeat within a set threshold, ensuring that peers searching for files are only directed to active peers.

\section*{Implementation Challenges}
Several challenges were addressed in the tracker implementation:
\begin{itemize}
	\item \textbf{Thread Safety:} Preventing concurrent shared data structure access with extensive use of mutexes.
	\item \textbf{Stale Peer Detection:} Implementing heartbeat mechanisms for efficient timeouts and cleanup to prevent peers from being directed to inactive nodes.
	\item \textbf{Efficient Peer Lists:} Optimised peer list format to include necessary connection information while keeping overhead low.
\end{itemize}

\section{File Server Component}
% Design considerations
% Implementation details
% File sharing and downloading processes
% Connection management

The File Server is the core peer functionality, responsible for sharing files, downloading from other peers, and managing local storage. It implements sophisticated algorithms for file chunk management, peer selection and concurrent transfers, allowing for efficient and resilient file distribution.

The File Server was designed with several important considerations. Concurrent operations allow a peer to simultaneously share and download multiple files without blocking. It has both UDP and TCP support, selecting TCP if available and UDP if not. It implements download mechanisms that can recover from peer failures and network disruptions through retry logic. Lastly, it manages system resources efficiently, ensuring that the system can handle many simultaneous connections without crashing.

The primary data structure is \texttt{FileServer struct, ADD CODE REFERENCE HERE} and is very extensive, tracking file metadata, maintaining references to active peers, local storage, and handlers for asynchronous communication. This structure allows the server to track all ongoing operations and coordinate between components efficiently. The system makes extensive use of Go's concurrency features like goroutines for parallel operations and channels for synchronised communication between different components.

When a file is shared, the \texttt{ShareFile, ADD CODE REFERENCE HERE} is executed. First, the system first generates a unique ID for the file using SHA-1 hashing of the content, ensuring consistent identification across the network. The file is then split into chunks of a fixed size (16MB by default), balancing transfer efficiency and memory usage. The system also calculates a has for each chunk to verify data integrity during transfers. A metadata structure is build with all the file information, including chunk and full hash, size data, and other file properties. This metadata is shared with the tracker, making the file discoverable by other peers.

The download process is one of the more complex implementations in the system and is quite sophisticated. It begins with file discovery, querying the tracker site for file metadata and a list of available peers in the \sloppy{ \texttt{DownloadFile, ADD CODE REFERENCE HERE}} method. The system then establishes connections to multiple peers, with fallback mechanisms in case of connection failures. The piece management strategy is based on BitTorrent's rarest-first approach, prioritising chunks that are least available in the swarm to ensure even distribution, maintaining swarm health. It tracks the active connections in the `peers` map and reuses them for transfers until they fail or disconnect. This simplifies the design but means there is no sophisticated management of connection pooling parameters like idle timeouts or pool sizes. The actual download process occurs in parallel, with multiple chunks being retrieved from different peers simultaneously. Each received chunk is cryptographically verified against the hash in the metadata to ensure data integrity. If a chunk verification fails, it is requested again. Lastly, the verified chunks are written to disk, and the file is reconstructed once all chunks are downloaded.

The File Server handles chunk requests and responses through methods that include retry logic, timeout handling, and checksum verification. The \texttt{downloadChunk, ADD CODE REFERENCE HERE} demonstrates this approach. These are implemented to ensure reliable transfers even in unstable network conditions. For large file transfers, the system implements adaptive timeouts based on chunk size, recognising that larger chunks require more transfer time. The implementation also includes mechanisms to limit concurrent downloads, preventing resource exhaustion while maintaining high throughput.

The system implements backoff strategies~\ref{lst:backoff-strategy} for failed connections/transfers, progressively increasing the wait time between retries to prevent overwhelming the network or the peer.

\section{Transport Layer}
% TCP/UDP dual protocol design
% NAT traversal implementation
% Protocol specifications and message formats
% Error handling and recovery

% Transport Layer
% The transport layer handles all network communication between peers, representing one of the most innovative aspects of the system. Unlike many P2P implementations that rely solely on TCP, this system implements a dual-protocol approach supporting both TCP and UDP, providing significant advantages in terms of performance and connectivity.
% The dual-protocol design is implemented through a Transport Manager that coordinates communication over both TCP and UDP. This manager presents a unified interface to the higher-level components while handling the complexities of protocol selection and fallback. This approach allows the system to optimize for different transfer scenarios, using TCP for reliability and UDP for speed. Critically, the UDP transport enables NAT traversal through hole punching, essential for establishing direct peer connections across networks. The system can also gracefully fall back from one protocol to the other when network conditions change or connections fail.
% The TCP transport provides reliable, ordered data transfer with several key optimizations. TCP keepalive ensures that connections remain active even during periods of inactivity, preventing premature disconnections due to NAT timeouts. Optimized buffer sizes are configured for large transfers, improving throughput for file chunks. TCP no-delay is enabled to improve interactive performance by reducing latency. A heartbeat mechanism detects dead connections promptly, allowing the system to recover and attempt alternative connections.
% The UDP transport implementation is more complex but provides crucial capabilities for a robust P2P system. It enables faster transfers for non-critical data and implements sophisticated NAT traversal through UDP hole punching. The UDP implementation handles large data transfers by segmenting content into appropriately sized packets to avoid fragmentation issues. It also implements reliability mechanisms, including acknowledgments and retransmissions, compensating for UDP's inherent unreliability. Timeout handling is comprehensive, monitoring request status and recovering from failed transmissions.
% The system defines a structured message protocol for communication, ensuring consistent exchanges across both transport types. Messages are typed and include appropriate payloads for different operations, such as chunk requests, chunk responses, and metadata exchanges. This protocol design provides type safety through Go's strong typing system and allows for future extensions without breaking backward compatibility.
% The NAT traversal implementation represents a significant technical achievement in the system. Using UDP hole punching, peers can establish direct connections even when both are behind NAT devices or firewalls. The process involves a coordinated exchange of messages, where peers simultaneously send "punch" packets to each other, creating temporary mappings in their respective NAT devices. When successful, these mappings allow direct communication without the need for a relay server. The implementation includes fallback mechanisms and retry logic to handle the variability of NAT behavior across different networks.
% IPv6 support was considered throughout the transport layer implementation, with the system capable of handling both IPv4 and IPv6 addresses. The transport layer automatically detects the address family in use and adjusts its behavior accordingly. This forward-looking design ensures the system will remain viable as networks transition to IPv6, while still supporting IPv4 networks during the extended transition period.



% Piece Management System
% The piece management system coordinates the efficient download of file chunks from multiple peers, ensuring optimal use of available bandwidth and resilience to peer failures. Inspired by BitTorrent's approach but with several enhancements, this component is critical to the system's performance and reliability.

% The piece manager was designed with several important considerations. First, it implements a "rarest-first" selection strategy, prioritizing chunks that are least available in the network to improve overall swarm health. Second, it coordinates parallel downloads from multiple peers, maximizing bandwidth utilization without overwhelming system resources. Third, it provides robust fault recovery, handling failed downloads and peer disconnections gracefully. Finally, it distributes download requests fairly across available peers, preventing any single peer from becoming a bottleneck.

% The core data structure maintains detailed information about each piece and its availability. Each piece record tracks its index, size, verification hash, current status (missing, requested, received, or verified), the peers that have it, and a dynamically calculated priority value. The piece manager itself maintains a map of these piece records, protected by a read-write mutex for thread safety during concurrent operations.

% The piece selection strategy represents a sophisticated algorithm combining rarest-first selection with randomization. The system first identifies all missing pieces that have available sources. It then sorts these candidates by priority, with rarer pieces receiving higher priority. To prevent the "thundering herd" problem where all peers request the same pieces simultaneously, the system introduces a controlled amount of randomness in the selection process. This approach balances efficient swarm operation with fair distribution of load across peers.

% The piece manager maintains comprehensive information about which peers have which pieces, updating this mapping as peers connect and disconnect. When a new peer connection is established, the system registers which pieces that peer has available, increasing the priority of pieces that remain rare. Conversely, when peers disconnect, the system removes them from piece availability records and resets the status of any in-progress pieces requested from that peer. This dynamic adjustment ensures the download continues smoothly despite peer churn.

% Status tracking is detailed and transactional, with pieces moving through well-defined states from missing to verified. When a piece's status changes, the system logs the transition for debugging purposes, providing visibility into the download process. This detailed tracking enables the system to know exactly which pieces are successfully downloaded, which are in progress, and which still need to be requested. The implementation also includes a completion check that verifies all pieces have been successfully downloaded and verified before finalizing the file.

% A significant enhancement over basic BitTorrent implementations is the system's adaptive request scheduling. Rather than requesting a fixed number of pieces, the system dynamically adjusts based on current network conditions and peer availability. In high-bandwidth environments with many peers, the system increases concurrent requests to maximize throughput. In more constrained scenarios, it reduces parallelism to avoid overwhelming resources. This adaptive approach ensures efficient operation across diverse network conditions.

% Storage System
% The storage system handles the persistent storage of file chunks and metadata on disk, implementing content-addressable storage principles for efficient retrieval and verification. This component ensures data integrity throughout the file sharing process and enables efficient reconstruction of files from downloaded chunks.

% The storage system was designed with several key considerations. It implements content addressing, using file hashes to create reliable references to stored data regardless of location. This approach enables robust verification and prevents duplication. The system efficiently manages individual chunks, providing fast storage and retrieval operations even for very large files. Metadata persistence is comprehensive, maintaining all information needed for future file sharing and verification. Finally, the system implements thorough verification mechanisms to ensure data integrity at every stage.

% Content-addressable storage is implemented through the PathTransform interface, which generates storage paths based on content hashes rather than arbitrary locations. This approach creates a unique, deterministic path for each file based on its content, enables efficient lookup without a centralized index, and prevents duplicates by identifying files by their content. The implementation uses SHA-1 hashing to generate these paths, though the design allows for future upgrades to stronger hash algorithms if needed.

% Chunk management is handled through optimized read and write operations. The reading process calculates appropriate chunk boundaries based on the requested index and file size, then performs an efficient direct read from the correct offset. The writing process includes verification, where the system optionally reads back written data to confirm it matches what was intended to be written. Both operations include retry logic with exponential backoff to handle temporary I/O issues, enhancing reliability on less stable storage systems.

% The system stores detailed metadata for each file to facilitate sharing and verification. This metadata includes the unique file identifier, information about chunk count and size, file extension and original path, cryptographic hashes for each chunk and the complete file, and total size information. This comprehensive metadata enables file integrity verification throughout the process, proper reassembly of chunks during downloads, efficient peer discovery through the tracker, and consistent file identification across the network.

% File and chunk integrity is ensured through a comprehensive hashing system. When a file is shared, the system calculates individual SHA-1 hashes for each chunk and a master hash for the entire file. These hashes are stored in the metadata and used during download to verify each received chunk. This verification system ensures that corrupted or tampered data is detected immediately and triggers re-downloading of affected chunks, maintaining end-to-end data integrity.

% The system includes optional encryption capabilities for sensitive data, implementing AES encryption in Counter (CTR) mode with secure initialization vector handling. This encryption can be applied selectively to files requiring additional security, with the encryption keys managed separately from the file data. The implementation provides transparent encryption and decryption of file data, maintaining the system's usability while adding security for sensitive content.
% Throughout the storage system implementation, careful attention was paid to error handling and recovery. Every file operation includes comprehensive error checking and appropriate recovery mechanisms, from retrying I/O operations to validating data integrity. This approach ensures that the system remains robust even when faced with unreliable storage media or unexpected system conditions.

The transport layer handles all network connections and communications between peers, allowing for both TCP and UDP protocols. This dual protocol design provides flexibility and optimises performance based on network conditions. The dual protocol design is implemented through a Transport Manager that coordinates communication over both TCP and UDP in~\ref{lst:transport-manager}. This manager presents a unified interface for the higher-level components and handles the underlying protocol implementation details. This approach allows the system to adapt to different network conditions, using TCP for reliability when available and UDP for speed. More importantly, the UDP transport enables NAT traversal through UDP-hole punching, essential for establishing connections between peers behind NAT devices.

The TCP transport provides reliable, ordered data transfer with several key optimisations:~\ref{lst:handleConn}. TCP keepalive ensures that connections remain active even during periods of inactivity, preventing unwanted disconnections. Buffer sizes are adjusted dynamically, optimising for large file transfers. TCP no-delay is enabled to reduce latency for small messages (TCP communications) improving responsiveness and interactivity. A heartbeat mechanism is implemented to periodically check the connection status and detect disconnections removing those peers promptly, allowing the system to recover and attempt alternate connections.

The UDP transport implementation is significantly more complex and provides crucial capabilities for a robust P2P system. It enables faster transfers for non-critical data and implements NAT traversal techniques to establish connections between peers behind NAT devices~\ref{lst:udp-holepunch}. It also implements reliability mechanisms, including acknowledgements and retransmissions, compensating for UDP's inherent unreliability.

The system defines a structured message format for all communications, ensuring consistent exchanges across both transport types~\ref{lst:message-format}. Messages are strongly typed with Golang's type system and include payloads with appropriate headers for different operations, such as chunk requests, chunk responses, and metadata exchanges. This structured approach simplifies parsing and processing of messages, reducing overhead.

\section{Piece Management System}
% Chunk selection strategies
% Parallel download coordination
% Implementation details
% Performance optimizations

The piece management system coordinates the efficient download of file chunks from multiple peers, ensuring optimal use of available bandwidth and resilience to peer failures. Inspired by BitTorrent's approach but with several enhancements, this component is critical to the system's performance and reliability. The core data structure for piece management maintains detailed information about each piece and its availability.

The piece manager was designed with various important considerations. It implements a \texttt{Rarest-First} selection strategy, prioritising chunks that are least available in the network to ensure availability of all chunks and improve swarm health. It coordinates parallel downloads from multiple peers, maximising bandwidth utilisation while maintaing acceptable memory usage. It handles failed downloads and peer disconnections gracefully \textbf{CHECK WHERE EXACTLY THIS IS AND ADD A CODE REFERENCE MAYBE.}

The core data structure maintains detailed information about each piece and its availabiltiy. Each piece record tracks its index, size, hash, current status (missing, requested, received, or verified), the peers that have the piece, and a priority value in the core data structure~\ref{lst:piece-info}. The \texttt{PieceManager} also keeps a comprehensive map of which peers have which pieces, updating this mapping as peers connect and disconnect and is protected by a read-write mutex for thread safety during concurrent operations on shared data structures.

As mentioned above, the piece selection based on the rarest-first strategy combined with randomness~\ref{lst:rarest-first}. It first identifies all missing pieces that have sources and sorts them by priority, with rarer pieces receiving higher priority. However, this causes an issue called the \texttt{Thundering Herd, \textbf{ADD A LIT REFERENCE}} where all peers request the same piece(s) simultaneously. To mitigate this, the system introduces a controlled amount of randomness in the selection process. When a new peer joins, the system registers which pieces that peer has available, increasing the priority of the pieces that remain rare. Conversely, when peers disconnect, the piece manager removes them from the map and resets all connections to that peer. This approach balances efficient swarm operation with fair load distribution amongst peers.

Status tracking is detailed and transactional. Pieces move through well-defined states from missing to verified~\ref{lst:status-tracking}. When a piece's status changes, the system logs the transition for debugging purposes. This tracking of pieces enables the system to kow exactly which pieces are successfully downloaded, which are in progress, and which still need to be requested. It also includes a completion check to verify all pieces before the final file is assembled~\ref{lst:piece-completion}. This even allows to have a download progress bar for a better user experience.

A significant enhancement over basic BitTorrent implementations is the system's adaptive request scheduling. Rather than requesting a fixed number of pieces, the system dynamically adjusts based on current network conditions and peer availability. In high-bandwidth environments with many peers, the system increases concurrent requests to maximize throughput. In more constrained scenarios, it reduces parallelism to avoid overwhelming resources. This adaptive approach ensures efficient operation across diverse network conditions. \textbf{Check this in server.go DownloadFile func 	const maxConcurrent = 10}

\section{Storage System}
% File chunking design
% Content addressing implementation
% Metadata management
% Verification mechanisms

The storage system handles the persistent storage of file chunks and metadata on disk, implementing content-addressable storage principles for efficient retrievl and verification. This component ensures data integrity throughout the file sharing process and enables efficient reconstruction of files from downloaded chunks.

It implements content addressing, using file hashes to create reliable references to stored data regardless of location. This approach guarantees file integrity providing robust verification and preventing duplication. The system 
\chapter{Technical Challenges and Solutions}
\section{Network NAT Traversal}
% The NAT problem in P2P
% UDP hole punching implementation
% Fallback mechanisms

\section{Concurrent Download Management}
% Thread safety concerns
% Goroutine coordination
% Bandwidth optimization

\section{Fault Tolerance Implementation}
% Handling peer disconnections
% Chunk verification and recovery
% Error handling strategies

\section{Security Considerations}
% Data integrity measures
% Attack surface analysis
% Mitigation strategies

\chapter{Testing and Evaluation}
\section{Testing Methodology}
% Test environments (VMs, network configurations)
% Test scenarios and conditions
% Measurement tools and techniques

\section{Performance Testing}
% Transfer speeds under various conditions
% CPU and memory utilization
% Network efficiency metrics

\section{Scalability Testing}
% Simulation of large peer networks (~10,000 users)
% Tracker performance under load
% System bottlenecks and pressure points

\section{Network Resilience Testing}
% Behavior under network disruptions
% Recovery from peer failures
% NAT traversal success rates

\section{Test Results and Analysis}
% Comprehensive performance data
% Statistical analysis
% Comparison with objectives

\chapter{Critical Evaluation}
\section{Achievements vs. Objectives}
% Review of project goals and outcomes

\section{Implementation Strengths}
% Notable achievements
% What worked particularly well
% Features to be proud of

\section{Implementation Weaknesses}
% Limitations of the current system
% Trade-offs made during development
% What went wrong and lessons learned

\section{Design Decisions Analysis}
% What would be done differently in hindsight
% Alternative approaches considered
% Justification of final choices

\chapter{Conclusion and Future Work}
\section{Project Summary}
% Key accomplishments and contributions

\section{Future Improvements}
% IPv6 support
% Enhanced security features
% GUI development
% Performance optimizations

\section{Final Reflections}
% Academic and practical significance
% Personal development and learning

\appendix
\chapter{Code Listings}

\section{Tracker code}
\begin{lstlisting}[language=Go, caption={TCP transport implementation}, label={lst:tracker-code}]
// FileInfo stores metadata about each shared file
type FileInfo struct {
    FileID      string    `json:"file_id"`
    Name        string    `json:"name"`
    Size        int64     `json:"size"`
    UploadedAt  time.Time `json:"uploaded_at"`
    Description string    `json:"description"`
    Categories  []string  `json:"categories"`
    NumPeers    int       `json:"num_peers"`
    Extension   string    `json:"extension"`
    NumChunks   int       `json:"num_chunks"`
    ChunkSize   int       `json:"chunk_size"`
    ChunkHashes []string  `json:"chunk_hashes"`
    TotalHash   string    `json:"total_hash"`
    TotalSize   int64     `json:"total_size"`
}

// Tracker maintains the state of files and peers
type Tracker struct {
    fileIndex    map[string]*FileInfo       // fileID -> file metadata
    peerIndex    map[string]map[string]bool // fileID -> peer addresses
    peerLastSeen map[string]time.Time       // peer address -> last heartbeat
    mu           sync.RWMutex
}
\end{lstlisting}

\section{Handle Announce code}
\begin{lstlisting}[language=Go, caption={Peer announce implementation}, label={lst:handle-announce}]
  func (t *Tracker) HandleAnnounce(w http.ResponseWriter, r *http.Request) {
    if r.Method != http.MethodPost {
        http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
        return
    }

    var announce struct {
        FileID      string   `json:"file_id"`
        PeerAddr    string   `json:"peer_addr"`
        Name        string   `json:"name"`
        Size        int64    `json:"size"`
        Description string   `json:"description"`
        Categories  []string `json:"categories"`
        Extension   string   `json:"extension"`
        NumChunks   int      `json:"num_chunks"`
        ChunkSize   int      `json:"chunk_size"`
        ChunkHashes []string `json:"chunk_hashes"`
        TotalHash   string   `json:"total_hash"`
        TotalSize   int64    `json:"total_size"`
    }

    if err := json.NewDecoder(r.Body).Decode(&announce); err != nil {
        http.Error(w, "Invalid request body", http.StatusBadRequest)
        return
    }

    t.mu.Lock()
    defer t.mu.Unlock()

    // Update file info if it doesn't exist or if it's changed
    existingFile, exists := t.fileIndex[announce.FileID]
    if !exists || existingFile.TotalHash != announce.TotalHash {
        t.fileIndex[announce.FileID] = &FileInfo{
            FileID:      announce.FileID,
            Name:        announce.Name,
            Size:        announce.Size,
            UploadedAt:  time.Now(),
            Description: announce.Description,
            Categories:  announce.Categories,
            Extension:   announce.Extension,
            NumChunks:   announce.NumChunks,
            ChunkSize:   announce.ChunkSize,
            ChunkHashes: announce.ChunkHashes,
            TotalHash:   announce.TotalHash,
            TotalSize:   announce.TotalSize,
        }
    }

    // Initialize peer list for this file if it doesn't exist
    if _, exists := t.peerIndex[announce.FileID]; !exists {
        t.peerIndex[announce.FileID] = make(map[string]bool)
    }

    // Add peer to the file's peer list
    t.peerIndex[announce.FileID][announce.PeerAddr] = true

    // Update peer's last seen timestamp
    t.peerLastSeen[announce.PeerAddr] = time.Now()

    // Update peer count
    t.fileIndex[announce.FileID].NumPeers = len(t.peerIndex[announce.FileID])

    log.Printf("Peer %s announced file %s (Total peers: %d)",
        announce.PeerAddr, announce.FileID, t.fileIndex[announce.FileID].NumPeers)

    w.WriteHeader(http.StatusOK)
}
\end{lstlisting}

\section{Backoff Strategy code}
\begin{lstlisting}[language=Go, caption={Backoff strategy implementation}, label={lst:backoff-strategy}]
// If we got here, all peers failed for this attempt
if attempt < maxRetries-1 {
    log.Printf("All peers failed for piece %d, attempt %d/%d. Last error: %v",
        chunkIndex, attempt+1, maxRetries, lastErr)
    time.Sleep(time.Second * time.Duration(attempt+1))
    continue
}
\end{lstlisting}

\section{Transport Manager}
\begin{lstlisting}[language=Go, caption={Transport Manager implementation}, label={lst:transport-manager}]
type TransportManager struct {
    transports []p2p.Transport
    rpcch      chan p2p.RPC
    mu         sync.RWMutex
}

func NewTransportManager(transports ...p2p.Transport) *TransportManager {
    tm := &TransportManager{
        transports: transports,
        rpcch:      make(chan p2p.RPC, 1024),
    }

    // Start consuming from all transports
    for _, t := range transports {
        go tm.forwardMessages(t)
    }

    return tm
}
\end{lstlisting}

\section{handleConn code}
\begin{lstlisting}[language=Go, caption={handleConn optimisations}, label={lst:handleConn}]
func (t *TCPTransport) handleConn(conn net.Conn, outbound bool) {
  // Set TCP keepalive
  if tcpConn, ok := conn.(*net.TCPConn); ok {
      tcpConn.SetKeepAlive(true)
      tcpConn.SetKeepAlivePeriod(heartbeatInterval)
      // Set TCP buffer sizes
      tcpConn.SetReadBuffer(1024 * 1024)  // 1MB read buffer
      tcpConn.SetWriteBuffer(1024 * 1024) // 1MB write buffer
      // Enable TCP no delay
      tcpConn.SetNoDelay(true)
  }
  // ...
}
\end{lstlisting}

\section{UDP Hole Punching code}
\begin{lstlisting}[language=Go, caption={UDP Hole Punch Implementation}, label={lst:udp-holepunch}]
func (t *UDPTransport) Dial(addr string) error {
  // ...

  addrs := strings.Split(addr, "|")
  log.Printf("Attempting UDP hole punching to addresses: %v", addrs)

  // Try hole punching for each address
  for _, address := range addrs {
      // ...

      // Create and send the PUNCH message
      punchMsg := fmt.Sprintf("PUNCH:%s:%d", publicIP, listenPort)
      log.Printf("Starting hole punching to %s (UDP: %s) with message: %s",
          targetAddr, udpAddr, punchMsg)

      // Send punch messages with exponential backoff
      for i := range 5 {
          // ...
      }
  }

  // Wait for connection or timeout
  select {
  case peerAddr := <-t.connectedCh:
      log.Printf("Connection established via UDP hole punching to %s", peerAddr)
      // ...
  case <-time.After(15 * time.Second):
      log.Printf("UDP hole punching timed out")
      return fmt.Errorf("UDP hole punching timeout")
  }
}
\end{lstlisting}

\section{Message Format code}
\begin{lstlisting}[language=Go, caption={Message Format}, label={lst:message-format}]
type Message struct {
  Type    string
  Payload interface{}
}

// Helper functions to ensure consistent message creation
func NewChunkRequestMessage(fileID string, chunk int) Message {
  return Message{
      Type: MessageTypeChunkRequest,
      Payload: MessageChunkRequest{
          FileID: fileID,
          Chunk:  chunk,
      },
  }
}
\end{lstlisting}

\section{Piece Manager data structure}
\begin{lstlisting}[language=Go, caption={Piece manager code}, label={lst:piece-info}]
type PieceInfo struct {
  Index    int
  Size     int
  Hash     string
  Status   PieceStatus
  Peers    map[string]bool // peers that have this piece
  Priority int             // higher number = higher priority
}

type PieceManager struct {
  pieces    map[int]*PieceInfo
  numPieces int
  mu        sync.RWMutex
}
\end{lstlisting}

\section{Rarest First Implementation}
\begin{lstlisting}[language=Go, caption={Rarest First code}, label={lst:rarest-first}]
func (pm *PieceManager) GetNextPieces(n int) []PieceInfo {
    // ...

    // Sort by priority (rarest first) and add some randomness to avoid all clients
    // requesting the same pieces simultaneously
    sort.Slice(candidates, func(i, j int) bool {
        // 80% of the time use priority, 20% of the time use random order
        if rand.Float32() < 0.8 {
            return candidates[i].Priority > candidates[j].Priority
        }
        return rand.Intn(2) == 0
    })

    // Return up to n pieces
    if len(candidates) > n {
        candidates = candidates[:n]
    }

    // Mark pieces as requested
    for _, piece := range candidates {
        pm.pieces[piece.Index].Status = PieceRequested
    }

    return candidates
}
\end{lstlisting}

\section{Piece Status Track Implementation}
\begin{lstlisting}[language=Go, caption={Piece Status Track Code}, label={lst:status-tracking}]
func (pm *PieceManager) MarkPieceStatus(index int, status PieceStatus) {
	pm.mu.Lock()
	defer pm.mu.Unlock()

	if piece, exists := pm.pieces[index]; exists {
		oldStatus := piece.Status
		piece.Status = status

		// Log status changes for debugging
		if oldStatus != status {
			statusNames := map[PieceStatus]string{
				PieceMissing:   "Missing",
				PieceRequested: "Requested",
				PieceReceived:  "Received",
				PieceVerified:  "Verified",
			}

			log.Printf("Piece %d status changed: %s -> %s",
				index,
				statusNames[oldStatus],
				statusNames[status])
		}
	}
}
\end{lstlisting}

\section{Pieces Completion Implementation}
\begin{lstlisting}[language=Go, caption={Pieces Completion Code}, label={lst:piece-completion}]
func (pm *PieceManager) IsComplete() bool {
	pm.mu.RLock()
	defer pm.mu.RUnlock()

	totalPieces := len(pm.pieces)
	verifiedCount := 0

	for _, piece := range pm.pieces {
		if piece.Status != PieceVerified {
			return false
		}
		verifiedCount++
	}

	// Debug logging for when we're close to completion
	if verifiedCount > 0 && verifiedCount >= totalPieces-5 {
		log.Printf("Completion check: %d/%d pieces verified", verifiedCount, totalPieces)
	}

	return true
}
\end{lstlisting}

\section{Content Addressable File Storage Implementation}
\begin{lstlisting}[language=Go, caption={Content Addressable File/Chunk Code}, label={lst:cas-storage}]
func generateFileHashes(filePath string, chunkSize int) ([]string, string, int64, error) {
file, err := os.Open(filePath)
if err != nil {
		return nil, "", 0, fmt.Errorf("failed to open file: %v", err)
	}
defer file.Close()

fileInfo, err := file.Stat()
if err != nil {
		return nil, "", 0, fmt.Errorf("failed to get file info: %v", err)
	}

totalSize := fileInfo.Size()
numChunks := (totalSize + int64(chunkSize) - 1) / int64(chunkSize)
chunkHashes := make([]string, int(numChunks))

// Calculate total file hash
file.Seek(0, 0)
totalHasher := sha1.New()
if _, err := io.Copy(totalHasher, file); err != nil {
		return nil, "", 0, fmt.Errorf("failed to calculate total hash: %v", err)
	}
totalHash := hex.EncodeToString(totalHasher.Sum(nil))

// Calculate chunk hashes
file.Seek(0, 0)
buf := make([]byte, chunkSize)
for i := int64(0); i < numChunks; i++ {
n, err := file.Read(buf)
if err != nil && err != io.EOF {
		return nil, "", 0, fmt.Errorf("failed to read chunk: %v", err)
	}
chunkHashes[i] = calculateHash(buf[:n])
}

return chunkHashes, totalHash, totalSize, nil
}


\chapter{Testing Data and Detailed Results}
% Detailed test results and analysis

\chapter{User Manual}
% System usage instructions

\renewcommand\bibname{References}
\begin{raggedright} % Prevents justification issues
	\bibliographystyle{unsrt}
	\bibliography{references}
\end{raggedright}

\end{document}
