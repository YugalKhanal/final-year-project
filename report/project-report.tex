\documentclass[12pt,a4paper]{report}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{booktabs}

% Code listing settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{Peer-to-Peer File Sharing System:\\A Robust and Scalable Implementation}
\author{Yugal Khanal\\2302704}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	The proliferation of distributed systems has led to increased interest in peer-to-peer (P2P) architectures for file sharing. This dissertation presents the design, implementation, and evaluation of a robust P2P file sharing system that addresses key challenges in scalability, fault tolerance, and security. The system implements a hybrid architecture combining centralized tracking with distributed file storage, featuring chunked file transfer, piece verification, and concurrent downloading capabilities.

	The implementation includes sophisticated features such as tracker-based peer discovery, UPnP port mapping for NAT traversal, and a comprehensive piece management system for handling large file transfers. Through extensive testing and evaluation, the system demonstrates reliable performance under various network conditions while maintaining data integrity and transfer efficiency.

	This work contributes to the field by implementing novel approaches to common P2P challenges, including peer availability management and fault-tolerant file transfers, while providing insights into the practical considerations of building distributed systems.

	\textbf{Keywords:} Peer-to-Peer Networks, Distributed Systems, File Sharing, Network Programming, Fault Tolerance
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\chapter{Introduction}
\section{Background and Motivation}
 [Discussion of the evolution of P2P systems and their role in modern networking]

The increase is the amount of Peer-to-Peer (P2P) file-sharing systems has changed digital content distribution. It's main advantage over it's traditional counterpart client-server model where a central authority manages file transfers is that P2P networks distribute this responsibility over to the peers in the network significantly increasing scalability and fault tolerance.

Data consumption has had a boom in recent years and because of the limitations of the client-server architecture such as high bandwidth costs, central points of failure etc., there has been a demand for efficient, secure and scalable file-sharing mechanisms. Modern P2P networks are the perfect solution to this. They incorporate advanced networking protocols, cryptographic security, and optimisation algorithms to enhance their performance and security. Enhanced scalability is achieved by distributing the file storage and transfer responsibilities among the peers in the network. This reduces the load on individual nodes and improves the overall efficiency of the system. Data redundancy and availability are also improved by storing multiple copies of the file across the network. These two are very strong points for P2P networks and any data distribution system would benefit from these features.

P2P networks have wider applications beyond just simple file-sharing. It's applications range from content distribution networks (CDNs) to blockchains and distributed cloud computing. There are also significant challenges in this approach. Some of them being data security, peer reliability and performance bottlenecks. This project aims to address some of these challenges by designing and implementing a secure, scalable and fault-tolerant P2P file-sharing system.


\section{Project Objectives}
 [Clear enumeration of project goals and success criteria]

The main objective of this project is to design and implement a robust and scalable P2P file-sharing system that addresses key challenges in scalability, fault tolerance, and security. The system will be evaluated based on the following criteria:

\begin{itemize}
	\item Develop a decentralised file-sharing system: The system should be able to handle a large number of peers and files while maintaining performance.
	\item Fault Tolerance: The system should be resilient to peer failures and network disruptions, ensuring reliable file transfers.
	\item Security: The system should implement secure communication protocols and data encryption to protect user data.
	\item Performance: The system should provide efficient file transfer mechanisms and low latency for peer interactions.
	\item Implement secure encryption mechanisms to safeguard data integrity.
\end{itemize}

\section{Evaluation Criteria}
 [Description of the metrics and methods used to evaluate the system]

\begin{itemize}
	\item System Performance: The system's performance will be evaluated it terms of latency, download speeds, system response and throughput efficiency under various network conditions.
	\item Scalability: The system's ability to handle increasing numbers of simultaneous connections and file-sharing requests without performance degradation.
	\item Fault Tolerance: Evaluation of the system's resilience to peer failures and disconnections, network disruptions, and data corruption.
	\item Security: Effefctiveness of the implemented encryption algorithms, peer authentication mechanisms and safeguards against malicious attacks.
	\item Resource Utilization: The system's resource consumption in terms of CPU, memory, and network bandwidth. How efficient the system is in using bandwidth and system resources.
\end{itemize}

By evaluating the system based on these criteria, we can determine the effectiveness of the implemented features and the overall performance of the P2P file-sharing system.
This project will hopefully also contribute to the advancement and highlight the importance of decentralized technologies.

\section{Problem Statement}
 [Detailed description of the challenges in P2P file sharing]

\section{Project Scope}
 [Outline of what the project encompasses and its boundaries]
Peer-to-peer file sharing is the distribuition

\chapter{Literature Review}
\section{History of P2P Systems}
 [Evolution of P2P architectures and protocols]

% At the start, file-sharing was exclusively done by client-server architecture via websites and FTP servers.
% This was a very inefficient way of sharing files as it was slow and expensive. The introduction of P2P networks changed this.
% P2P networks are decentralised and distribute the file-sharing responsibilities among the peers in the network.
% This significantly increased the scalability and fault tolerance of the system. The first P2P network was Napster
% which was a centralised P2P network. This was followed by Gnutella which was a decentralised P2P network.
% BitTorrent was the next big thing in P2P networks. It introduced a new way of file-sharing by breaking the files
% into smaller chunks and sharing them among the peers. This significantly increased the efficiency, scalability and reliability
% of the system. Modern P2P networks have incorporated advanced networking protocols, cryptographic security, and optimisation
% algorithms to enhance their performance and security.

Early peer-to-peer (P2P) file sharing systems showed up in the late 1990s and evolevd rapidly throught the 2000s. Everything started with Napster (1999), created by Shawn Fanning, that allowed users to share music files (MP3) \cite{wikipedia-p2p}. Napster used a centralised server to index available files while the actual file transfers were done directly between user computers (peers). This centralised index made Napster easy to use and very popular. At its peak, Napster had 80 million registered users \cite{wikipedia-napster} which also became its downfall as it was down in 2001 for copyright infringement.

Following Napster, Gnutella was released in 2000 as the very first fully decentralised P2P file sharing network. With Gnutella, the centralised index server was gone; instead, each peer acted equally as a client and a server. The file requests were broadcasted to all peers. At the start, this was great for reliability. However, scaling became a problem as the network grew as flooding each query to all peers caused high overhead and slow searching. Around the same time, other systems like FastTrack (Kazaa, 2001) which became the most popular P2P protocol at the time, used supernodes as indexes to improve scalability. These supernodes were a subset of peers with high-bandwidth that acted as a proxy for other peers \cite{wikipedia-fasttrack}. This combined the central index's efficiency with a decentralised system for actual file transfers. This reduced the overhead of flooding queries to all peers. Similarly, eDonkey2000 (2000) relied on multiple servers for indices, and it's later client eMule (2002) implemented a Kademlia distributed hash table (Kad network) to enable serverless operation.

By the early 2000s,P2P networks had many versions. Some with unstructured networks like Gnutella, structured networks with distributed indices, and hybrid models. The early success and failure of Napster allowed for new P2P file-sharing networks and their software client counterparts \cite{early-2000s-p2p-state}. Since the newer P2P systems don't rely on any central server by using a Distributed Hash Table (DHT), it is much harder for law enforcements to shut it down. The concept of P2P networks has been around for long time. The fundamental Internet Protocols like TCP/IP with its applications in FTP, HTTP, etc. are all based on host-to-host communication where all hosts can request data from all other hosts \cite{early-2000s-p2p-state}. P2P networks are an evolution/extention of these principles, building application-level networks on top of the Internet that directly connect users to each other to freely share any kind of data.

BitTorrent was the biggest milestone in P2P network history, introduced by Bram Cohen in 2001. BitTorrent's totally new and innovative approach to file sharing - discussed in more detail in the next section - immensely improved scalability and efficiency for distributing large files. By utilising swarms of users to concurrently download and upload pieces of a file, BitTorrent was able to reduce the load on individual peers and increase the overall speed of file transfers. This made BitTorrent the most dominant P2P file sharing protocol by the mid-2000s and is still the most popular P2P protocol today \cite{BitTorrent-most-popular}. To summarise, the evolution of P2P file sharing moved from a central index model like Napster, to a purely decentralised but inefficient model like Gnutella, to hybrid and structured network like FastTrack and eDonkey2000, and finally ending up in the efficient and scalable BitTorrent model. These advancements in P2P technology were driven by the need for a lack of central bottlenecks, improved discovery and download speeds and increase reliability (robutness) of the file sharing networks agains failures and legal threats.


\section{BitTorrent Protocol Analysis}
 [Detailed examination of BitTorrent's approach]

BitTorrent is a peer-to-peer protocol designed for efficient distribution of large files by utilising swarms (the collective group of peers sharing a particular file) to download and upload pieces of the file concurrently. Under the hood, BitTorrent breaks each file into many small pieces called chunks and distributes them among the peers in a swarm, so all participants can download and exchange different pieces with each other simultaneously \cite{BitTorrent-spec}. A user requires a torrent descriptor file (.torrent) or a magne link, which contains metadata including a unique info-has identifies for the content and optionally addresses of tracker servers - coordinatin servers that help peers fine each other \cite{BitTorrent-spec}. When any BitTorrent client loads this metadata, it joins the swarm by contacting the tracker server or using a Distributed Hash Table (DHT) to discover other peers. Peers in the swarm that have the complete files are called seeders, while those that are still downloading are called leechers or just peers. When the leecher downloads any piece, it begins to upload that piece to others immediately after, contributing to the swarm. This design means that the content distribution load is shared among all peers, and the more peers there are, the faster the download speed can be. With enough peers, a file's original source only needs to provide one full copy, after which the swarm can continue to share the file without the original source's involvement. This decentralised sharing content makes BitTorrent highly scalable.

In a BitTorrent network, at the start, file pieces are downloaded in a strategic order using the \textit{rarest-first}. Peers prioritise requesting pieces that are rarest in the swarm (pieces that fewest other peers have), making those pieces more widely replicated early to ensure availability.

\section{Modern P2P Applications}
 [Survey of current P2P implementations]

\section{Security Challenges in P2P Networks}
 [Analysis of security considerations]

\section{Distributed Hash Tables and Peer Discovery}
 [Review of peer discovery mechanisms]

\chapter{System Architecture}
\section{High-Level Design}
 [System overview with architectural diagrams]

\section{Component Overview}
 [Detailed description of system components]

\section{Network Protocol Design}
 [Protocol specifications and communication patterns]

\section{Data Flow Architecture}
 [Data flow diagrams and explanations]

\section{Storage System Design}
 [File storage and management architecture]

\chapter{Implementation Details}
\section{Tracker Implementation}
 [Details of the tracking system]

\section{Peer Discovery and Management}
 [Peer handling mechanisms]

\section{File Chunking and Transfer Protocol}
 [File transfer implementation details]

\section{Concurrent Download Management}
 [Concurrency handling approaches]

\section{Error Handling and Recovery}
 [Error management strategies]

\section{Security Implementation}
 [Security measures and protocols]

\chapter{Technical Challenges and Solutions}
\section{Network NAT Traversal}
 [NAT handling implementation]

\section{File Integrity Verification}
 [Data verification mechanisms]

\section{Peer Availability Management}
 [Peer management strategies]

\section{Performance Optimization}
 [Performance improvements]

\section{Fault Tolerance Implementation}
 [Fault handling approaches]

\chapter{Testing and Evaluation}
\section{Performance Metrics}
 [Performance testing results]

\section{Scalability Testing}
 [Scalability analysis]

\section{Network Resilience}
 [Network testing results]

\section{Security Testing}
 [Security evaluation]

\section{User Experience Testing}
 [Usability assessment]

\chapter{Conclusion and Future Work}
\section{Project Achievements}
 [Summary of accomplishments]

\section{Limitations}
 [Project limitations]

\section{Future Improvements}
 [Potential enhancements]

\section{Final Reflections}
 [Concluding thoughts]

\appendix
\chapter{Code Listings}
\section{Core Components}
% Example code listing:
\begin{lstlisting}[language=Go, caption=Tracker Implementation]
type Tracker struct {
    fileIndex    map[string]*FileInfo
    peerIndex    map[string]map[string]bool
    peerLastSeen map[string]time.Time
    mu           sync.RWMutex
}
\end{lstlisting}

\chapter{Testing Data}
 [Detailed test results and analysis]

\chapter{User Manual}
 [System usage instructions]

\renewcommand\bibname{References}
\begin{raggedright} % Prevents justification issues
	\bibliographystyle{unsrturl}
	\bibliography{references}
\end{raggedright}



\end{document}
