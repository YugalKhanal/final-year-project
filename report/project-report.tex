\documentclass[12pt,a4paper]{report}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{booktabs}
\usepackage{xcolor}
% \usepackage{titlesec}
\usepackage{parskip}
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% Code listing settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{xcolor}

% Define inline code styling
\lstdefinestyle{inlineCode}{
    backgroundcolor=\color{backcolour},
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    columns=flexible
}

% Define custom inline code command
\newcommand{\code}[1]{\lstinline[style=inlineCode]!#1!}


\title{Peer-to-Peer File Sharing System:\\A Robust and Scalable Implementation}
\author{Yugal Khanal\\2302704}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	The proliferation of distributed systems has led to increased interest in peer-to-peer (P2P) architectures for file sharing. This dissertation presents the design, implementation, and evaluation of a robust P2P file sharing system that addresses key challenges in scalability, fault tolerance, and security. The system implements a hybrid architecture combining centralized tracking with distributed file storage, featuring chunked file transfer, piece verification, and concurrent downloading capabilities.

	The implementation includes sophisticated features such as tracker-based peer discovery, UPnP port mapping for NAT traversal, and a comprehensive piece management system for handling large file transfers. Through extensive testing and evaluation, the system demonstrates reliable performance under various network conditions while maintaining data integrity and transfer efficiency.

	This work contributes to the field by implementing novel approaches to common P2P challenges, including peer availability management and fault-tolerant file transfers, while providing insights into the practical considerations of building distributed systems.

	\textbf{Keywords:} Peer-to-Peer Networks, Distributed Systems, File Sharing, Network Programming, Fault Tolerance
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\chapter{Introduction}
\section{Background and Motivation}
The increase is the amount of Peer-to-Peer (P2P) file-sharing systems has changed digital content distribution. It's main advantage over it's traditional counterpart client-server model where a central authority manages file transfers is that P2P networks distribute this responsibility over to the peers in the network significantly increasing scalability and fault tolerance.

Data consumption has had a boom in recent years and because of the limitations of the client-server architecture such as high bandwidth costs, central points of failure etc., there has been a demand for efficient, secure and scalable file-sharing mechanisms. Modern P2P networks are the perfect solution to this. They incorporate advanced networking protocols, cryptographic security, and optimisation algorithms to enhance their performance and security. Enhanced scalability is achieved by distributing the file storage and transfer responsibilities among the peers in the network. This reduces the load on individual nodes and improves the overall efficiency of the system. Data redundancy and availability are also improved by storing multiple copies of the file across the network. These two are very strong points for P2P networks and any data distribution system would benefit from these features.

P2P networks have wider applications beyond just simple file-sharing. It's applications range from content distribution networks (CDNs) to blockchains and distributed cloud computing. There are also significant challenges in this approach. Some of them being data security, peer reliability and performance bottlenecks. This project aims to address some of these challenges by designing and implementing a secure, scalable and fault-tolerant P2P file-sharing system.


\section{Project Objectives}
 [Clear enumeration of project goals and success criteria]

The main objective of this project is to design and implement a robust and scalable P2P file-sharing system that addresses key challenges in scalability, fault tolerance, and security. The system will be evaluated based on the following criteria:

\begin{itemize}
	\item Develop a decentralised file-sharing system: The system should be able to handle a large number of peers and files while maintaining performance.
	\item Fault Tolerance: The system should be resilient to peer failures and network disruptions, ensuring reliable file transfers.
	\item Security: The system should implement secure communication protocols and data encryption to protect user data.
	\item Performance: The system should provide efficient file transfer mechanisms and low latency for peer interactions.
	\item Implement secure encryption mechanisms to safeguard data integrity.
\end{itemize}


\section{Evaluation Criteria}
 [Description of the metrics and methods used to evaluate the system]

\begin{itemize}
	\item System Performance: The system's performance will be evaluated it terms of latency, download speeds, system response and throughput efficiency under various network conditions.
	\item Scalability: The system's ability to handle increasing numbers of simultaneous connections and file-sharing requests without performance degradation.
	\item Fault Tolerance: Evaluation of the system's resilience to peer failures and disconnections, network disruptions, and data corruption.
	\item Security: Effefctiveness of the implemented encryption algorithms, peer authentication mechanisms and safeguards against malicious attacks.
	\item Resource Utilization: The system's resource consumption in terms of CPU, memory, and network bandwidth. How efficient the system is in using bandwidth and system resources.
\end{itemize}

By evaluating the system based on these criteria, we can determine the effectiveness of the implemented features and the overall performance of the P2P file-sharing system.
This project will hopefully also contribute to the advancement and highlight the importance of decentralized technologies.

\section{Problem Statement}
 [Detailed description of the challenges in P2P file sharing]

\section{Project Scope}
 [Outline of what the project encompasses and its boundaries]
Peer-to-peer file sharing is the distribuition

\chapter{Literature Review}
\section{History of P2P Systems}
Early peer-to-peer (P2P) file sharing systems showed up in the late 1990s and evolevd rapidly throught the 2000s. Everything started with Napster (1999), created by Shawn Fanning, that allowed users to share music files (MP3) \cite{wikipedia-p2p}. Napster used a centralised server to index available files while the actual file transfers were done directly between user computers (peers). This centralised index made Napster easy to use and very popular. At its peak, Napster had 80 million registered users \cite{wikipedia-napster} which also became its downfall as it was down in 2001 for copyright infringement.

Following Napster, Gnutella was released in 2000 as the very first fully decentralised P2P file sharing network. With Gnutella, the centralised index server was gone; instead, each peer acted equally as a client and a server. The file requests were broadcasted to all peers. At the start, this was great for reliability. However, scaling became a problem as the network grew as flooding each query to all peers caused high overhead and slow searching. Around the same time, other systems like FastTrack (Kazaa, 2001) which became the most popular P2P protocol at the time, used supernodes as indexes to improve scalability. These supernodes were a subset of peers with high-bandwidth that acted as a proxy for other peers \cite{wikipedia-fasttrack}. This combined the central index's efficiency with a decentralised system for actual file transfers. This reduced the overhead of flooding queries to all peers. Similarly, eDonkey2000 (2000) relied on multiple servers for indices, and it's later client eMule (2002) implemented a Kademlia distributed hash table (Kad network) to enable serverless operation.

By the early 2000s,P2P networks had many versions. Some with unstructured networks like Gnutella, structured networks with distributed indices, and hybrid models. The early success and failure of Napster allowed for new P2P file-sharing networks and their software client counterparts \cite{early-2000s-p2p-state}. Since the newer P2P systems don't rely on any central server by using a Distributed Hash Table (DHT), it is much harder for law enforcements to shut it down. The concept of P2P networks has been around for long time. The fundamental Internet Protocols like TCP/IP with its applications in FTP, HTTP, etc. are all based on host-to-host communication where all hosts can request data from all other hosts \cite{early-2000s-p2p-state}. P2P networks are an evolution/extention of these principles, building application-level networks on top of the Internet that directly connect users to each other to freely share any kind of data.

BitTorrent was the biggest milestone in P2P network history, introduced by Bram Cohen in 2001. BitTorrent's totally new and innovative approach to file sharing - discussed in more detail in the next section - immensely improved scalability and efficiency for distributing large files. By utilising swarms of users to concurrently download and upload pieces of a file, BitTorrent was able to reduce the load on individual peers and increase the overall speed of file transfers. This made BitTorrent the most dominant P2P file sharing protocol by the mid-2000s and is still the most popular P2P protocol today \cite{BitTorrent-most-popular}. To summarise, the evolution of P2P file sharing moved from a central index model like Napster, to a purely decentralised but inefficient model like Gnutella, to hybrid and structured network like FastTrack and eDonkey2000, and finally ending up in the efficient and scalable BitTorrent model. These advancements in P2P technology were driven by the need for a lack of central bottlenecks, improved discovery and download speeds and increase reliability (robutness) of the file sharing networks agains failures and legal threats.


\section{BitTorrent Protocol Analysis}
BitTorrent is a peer-to-peer protocol designed for efficient distribution of large files by utilising swarms (the collective group of peers sharing a particular file) to download and upload pieces of the file concurrently. Under the hood, BitTorrent breaks each file into many small pieces called chunks and distributes them among the peers in a swarm, so all participants can download and exchange different pieces with each other simultaneously \cite{bep_0003}. A user requires a torrent descriptor file (.torrent) or a magne link, which contains metadata including a unique info-has identifies for the content and optionally addresses of tracker servers - coordinatin servers that help peers fine each other \cite{bep_0003}. When any BitTorrent client loads this metadata, it joins the swarm by contacting the tracker server or using a Distributed Hash Table (DHT) to discover other peers. Peers in the swarm that have the complete files are called seeders, while those that are still downloading are called leechers or just peers. When the leecher downloads any piece, it begins to upload that piece to others immediately after, contributing to the swarm. This design means that the content distribution load is shared among all peers, and the more peers there are, the faster the download speed can be. With enough peers, a file's original source only needs to provide one full copy, after which the swarm can continue to share the file without the original source's involvement. This decentralised sharing content makes BitTorrent highly scalable.

In a BitTorrent network, at the start, file pieces are downloaded in a strategic order using the \textit{rarest-first}. Peers prioritise requesting pieces that are rarest in the swarm (pieces that fewest other peers have), making those pieces more widely replicated early to ensure availability. This strategy ensures that no single piece/peer becomes a bottleneck in the swarm. BitTorrent also implements a tit-for-tat inspired choking algorithm to encourage cooperation and as it's incentive mechanism. In practice, each client monitors its other peers' upload/download rates and "chokes" (temporarily stops sending data to, downloading can still take place) peers that are not sharing, which unchoking peers that do send data \cite{choking}. Consequently, a peer that uploads more is allowed to download more from others. This leads to faster overall performace for peers that share. This tit-for-tat mechanism is crucial in discouraging free-riders and keeps the swarm healthy. BitTorrent also uses \textit{optimistic unchoking} to randomly unchoke some connections, allowing new and temporarily idle peers to gain pieces \cite{choking}. This also gives a chance to previously choked peers to state that they are willing to share. Because there is no central scheduler, each peer automatically maximises its own download rate by selectively uploading to partners that reciprocate. Doing this also stabilises the download speed, giving peers a consistent download rate.

BitTorrent's swarming approach of downloading pieces from multiple peers in parallel means that the collective bandwidth of the swarm increases as more peers join, greatly improving download efficiency. To put it simply, the swarm's average throughput scales with the number of peers - a sharp contrast and advantage to traditional client-server downloads limited by a server's capacity.

Initially, BitTorrent relied on centralised servers/trackers for peer discovery, but this presented reliability and availability risks (if a tracker went offline, the swarm could become unreachable). To address this, a \textbf{Distributed Hash Table (DHT)} system was introduced (BitTorrent Enhancement Protocol (BEP) 5) to decentralise peer discovery. The DHT is composed of nodes and stores the location of peers. BitTorrent clients include a DHT node, which is used to contact other nodes in the DHT to get the location of peers to download from using the BitTorrent protocol \cite{bep_0005}. The DHT is based on the Kademlia algorithm. In Kademlia, the distance metric is XOR and the result is interpreted as an unsigned integer. distance(A, B) = $|\text{A xor B}|$ smaller values are closer. The distance metric is used to compare two node IDs or a node ID and an infohash for "closeness" \cite{bep_0005}. Peers publish and retrieve torrent swarm information on the DHT by using the torrent's infohash as the key. To find peers for a torrent, a node performs \texttt{get\_peers} query on the DHT with the infohash. The DHT nodes responsible for that key will respond with a list of IP and ports for peers in the swarm. For any peer joining the swarm, the peer uses \texttt{announce\_peer} along with the torrent infohash to announce its presence to the DHT so other nodes can find it \cite{bep_0005}. This way the DHT functions as a decentralised "tracker", allowing peers to find each other without any central server.

In addition to the DHT, BitTorrent also utilises \textbf{Peer Exchange (PEX)} and \textbf{Local Peer Discovery (LDP)} to further improve peer discovery. PEX is an alternative peer discovery mechanish for swarms once peers have bootstrapped via other mechanishms such as DHT \cite{bep_0011}. PEX allows peers to directly exchange peer lists with each other, enabling them to discover new peers quickly without needing a tracker or DHT. LDP allows peers on the same local network (on the same LAN) to discover each other using multicast (http over udp-multicast) \cite{bep_0014}. It's aim is to minimise the traffic through the ISP's channel use the higher LAN bandwidth instead. Together with the DHT, PEX and LDP provide a robust and decentralised peer discovery mechanism for BitTorrent.

Through the combination of efficient piece distribution, tit-for-tat incentive mechanism (choking/unchoking), and multiple peer discovery methods, BitTorrent achieves both very high performance and reliability. BitTorrent remains the most widely used P2P protocol today having inspired many other distributed systems.

\section{Modern P2P Applications}
Although P2P technology began with file sharing, today it is used in a diverse array of modern applications beyond just simple file sharing. One such domain is \textbf{Blockchain} and \textbf{Cryptocurrencies}. They fundamentally operate as P2P networks. In the original white paper for Bitcoin, introduced in 2008, it is described as a "peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions" and a Bitcoin to be an electronic coin which is a chain of digital signatures. In a Blockchain network like Bitcoin, each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin without a central authority \cite{bitcoin}. Each node also maintains a copy of the ledger and participates in a proof-of-work protocol to agree on the next block of transactions. The proof-of-work involves finding a value that, when hashed with the block's data, produces a hash with some number of leading zeros \cite{bitcoin}. This decentralised design makes the system resilient and resistant to censorship and attacks. It even has an incentive mechanish to encourage participation in the form of newly minted coins or transaction fees, thereby keeping the network running smoothly. Multiple decentralised applications have been built on blockchain P2P networks (Ethereum being the most popular).

Decentralised data storage and content distribution is another area where P2P networks are used. IPFS (InterPlanetary File System) is a notable example of modern P2P protocol for distributed file storage. It's a protocol and network designed to create a content-addressable, peer-to-peer method of storing and sharing hypermedia in a distributed file system \cite{IPFS}. It forms a global, decentralised file system in which content is addressed by a unique hash. Upon a request for content, the IPFS network uses a DHT to locate peers that have the content and retrieves it from the fastest available sources. This approach allows content to be shared across multiple nodes, making content delivery efficient and reliable (files remain available even if some nodes disconnect, just like in BitTorrent). Many newer applications, especially Web3 applications, leverage IPFS or similar P2P protocols for content distribution without relying on a central server.

From file-sharing to cryptocurrencies and decentralised storage, modern P2P networks demonstrate the versatility and power of decentralised systems. By removing central servers, these applications gain fault-tolerance, increased performance and availability. The principles of P2P networking have been applied to a wide range of applications, and the technology continues to evolve with new use cases and innovations.

\section{Security Challenges in P2P Networks}
Along with the various advantages of P2P networks, they also present significant security and privacy challenges. It's decentralised nature and direct peer-to-peer communication makes enforcing security policies and protecting users more difficult compared to traditional centralisd systems.

\textbf{Privacy and Anonymity} is the primary challenge. In P2P networks, peers' IP addresses are exposed to others in the newtork, meaning user activiy can be monitored. True anonymity is difficult to achieve in P2P networks, as the IP addresses of peers must be published in order to connect and exchange data. This makes it easy for organisations to join swarms to log peer IP addresses and ban those IPs from joining P2P newtorks. Users can use VPNs or other IP masking techniques to hide their true IP address, but this merely shifts the trust to the VPN provider and is not foolproof. As a result, traditional P2P networks offer little built-in anonymity. Some specalised networks emerged to address this issue, like I2P, Freenet, and Tor etc. These networks route traffic through multiple nodes to obfuscate the source and destination of data, providing a higher level of privacy and anonymity. However, they trade off efficiency and are not widely adopted for general file sharing.

\textbf{Malware and Content Integrity} is another major concern. P2P networks are often used to distribute pirated content, which can be a vector for malware. Malicious users can upload malware disguised as legitimate files with no party checking it for authenticity. Users can unknowingly download these files and install malware thinking it is a legitimate file. P2P networks can become a breeding ground for trojans and viruses because of the ease of injection. Attacker can also flood the networks with fake or corrupted files due to the lack of verification \cite{security} \textbf{RECHECK THISSSSSSSSSSSSSSS}.

\chapter{System Design and Implementation}
\section{Architecture Overview}
% High-level design and key components
% Design principles and patterns
% Development environment (VMs, networking setup, etc.)

This P2P file sharing system implements a hybrid architecture that combines \textbf{centralised tracking} for peer discovery with \textbf{decentralised file transfer} among peers. This design provides the scalability benefits of distributed systems while maintaining efficient peer discovery through a central coordinator. At a high level, the system consists of the following core components:

\begin{itemize}
	\item \textbf{Tracker:} A lightweight centralised server that maintains metadata about files and coordinates peer discovery. The tracker does not store any file content and does not participate in file transfers, only facilitating peer connections.
	\item \textbf{File Server:} The core component of the system running on each peer that handles file storage, chunking, storage and downloading. It implements BitTorrent inspired piece management and chunking strategies for efficient file transfers.
	\item \textbf{Transport Layer:} A dual protocol system that supports both TCP and UDP for optimal file transfers and NAT traversal.
	\item \textbf{Storage System:} Manages the persistent storage of file chunks and metadata on disk. Maintains data integrity and verification mechanisms along with content addressing and file reconstruction.
	\item \textbf{Piece Manager:} Coordinates the parallel downloading and verification of individual file chunks using a variety of piece selection strategies for optimal performance.
\end{itemize}

File transfers are inspired by BitTorrent's approach, where files are divided into chunks, allowing for parallel downloading from multiple peers simultaneously. Each chunk is hashed and verified to maintain data integrity. The combination of all core components ensures a robust and scalable file-sharing experience for users.

Certain design principles and patterns dictated the system's architecture:
\begin{itemize}
	\item \textbf{Fault Tolerance:} The system is designed to be resilient to peer failures and network disruptions. The system can handle peer disconnections, network failures, and data corruption gracefully through techniques like retry logic, error recovery, and peer connection strategies.
	\item \textbf{Scalability:} By distributing file storage and transfer responsibilities among peers themselves, the system can scale to handle many simultaneous users and large files with no central bottleneck.
	\item \textbf{Performance Optimisation:} The system includes optimisations like parallel downloads, piece prioritisation (rarest first), etc. to maximise file transfer speeds.
	\item \textbf{Protocol Flexibility:} The dual TCP and UDP transport layer allows the system to adapt to different network conditions when NAT traversal is required.
	\item \textbf{Data Integrity:} \sloppy{Hashing and verification mechanisms are implemented to ensure that file chunks are not corrupted during transfer and tampered with.}
\end{itemize}

The system was developed in Go (Golang), chosen for its efficiency, excellent concurrency model with goroutines and channels, strong standard networking (among others) libraries, and cross-platform compatibility. The development environment consisted of:
\begin{itemize}
	\item Go 1.23 for the programming language on the local machine running NixOS(btw).
	\item Multiple virtual machines (VMs) running Ubuntu to simulate a distributed network.
	\item Network configuration with various NAT setups to test NAT traversal.
	\item Comprehensive logging for debugging and monitoring.
\end{itemize}

\section{Tracker Component}
% Design considerations
% Data structures and algorithms
% HTTP endpoints and protocol
% Implementation challenges and solutions

The tracker servers as a lightweight coordination service that helps peers discover each other and share file metadata. It maintains an index of available files and the peers sharing them.

\section*{Design Considerations}

\section{File Server Component}
% Design considerations
% Implementation details
% File sharing and downloading processes
% Connection management

\section{Transport Layer}
% TCP/UDP dual protocol design
% NAT traversal implementation
% Protocol specifications and message formats
% Error handling and recovery

\section{Piece Management System}
% Chunk selection strategies
% Parallel download coordination
% Implementation details
% Performance optimizations

\section{Storage System}
% File chunking design
% Content addressing implementation
% Metadata management
% Verification mechanisms

\chapter{Technical Challenges and Solutions}
\section{Network NAT Traversal}
% The NAT problem in P2P
% UDP hole punching implementation
% Fallback mechanisms

\section{Concurrent Download Management}
% Thread safety concerns
% Goroutine coordination
% Bandwidth optimization

\section{Fault Tolerance Implementation}
% Handling peer disconnections
% Chunk verification and recovery
% Error handling strategies

\section{Security Considerations}
% Data integrity measures
% Attack surface analysis
% Mitigation strategies

\chapter{Testing and Evaluation}
\section{Testing Methodology}
% Test environments (VMs, network configurations)
% Test scenarios and conditions
% Measurement tools and techniques

\section{Performance Testing}
% Transfer speeds under various conditions
% CPU and memory utilization
% Network efficiency metrics

\section{Scalability Testing}
% Simulation of large peer networks (~10,000 users)
% Tracker performance under load
% System bottlenecks and pressure points

\section{Network Resilience Testing}
% Behavior under network disruptions
% Recovery from peer failures
% NAT traversal success rates

\section{Test Results and Analysis}
% Comprehensive performance data
% Statistical analysis
% Comparison with objectives

\chapter{Critical Evaluation}
\section{Achievements vs. Objectives}
% Review of project goals and outcomes

\section{Implementation Strengths}
% Notable achievements
% What worked particularly well
% Features to be proud of

\section{Implementation Weaknesses}
% Limitations of the current system
% Trade-offs made during development
% What went wrong and lessons learned

\section{Design Decisions Analysis}
% What would be done differently in hindsight
% Alternative approaches considered
% Justification of final choices

\chapter{Conclusion and Future Work}
\section{Project Summary}
% Key accomplishments and contributions

\section{Future Improvements}
% IPv6 support
% Enhanced security features
% GUI development
% Performance optimizations

\section{Final Reflections}
% Academic and practical significance
% Personal development and learning

\appendix
\chapter{Code Listings}
% Key code examples with explanations

\chapter{Testing Data and Detailed Results}
% Detailed test results and analysis

\chapter{User Manual}
% System usage instructions

\renewcommand\bibname{References}
\begin{raggedright} % Prevents justification issues
	\bibliographystyle{unsrt}
	\bibliography{references}
\end{raggedright}

\end{document}
